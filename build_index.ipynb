{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_arxiv_papers\n",
    "papers=[]\n",
    "papers = fetch_arxiv_papers(\"Language Model\", 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark',\n",
       " 'ASGO: Adaptive Structured Gradient Optimization',\n",
       " 'MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search',\n",
       " 'Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning',\n",
       " 'MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams',\n",
       " 'Dynamic Motion Blending for Versatile Motion Editing',\n",
       " 'From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models',\n",
       " 'MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion',\n",
       " 'Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy',\n",
       " 'TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews',\n",
       " 'AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language Model for Lung Nodule Malignancy Prediction',\n",
       " 'AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports',\n",
       " 'Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging',\n",
       " 'Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions',\n",
       " 'IAP: Improving Continual Learning of Vision-Language Models via Instance-Aware Prompting',\n",
       " 'What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical Study and Beyond',\n",
       " 'LLPut: Investigating Large Language Models for Bug Report-Based Input Generation',\n",
       " 'Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models',\n",
       " 'Low-resource Information Extraction with the European Clinical Case Corpus',\n",
       " 'A Theoretical Framework for Prompt Engineering: Approximating Smooth Functions with Transformer Prompts']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the titles of the papers\n",
    "[paper[\"title\"] for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document \n",
    "\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        content = (\n",
    "            f\"Title: {paper['title']}\\n\"\n",
    "            f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            f\"Summary: {paper['summary']}\\n\"\n",
    "            f\"Published: {paper['published']}\\n\"\n",
    "            f\"Journal Reference: {paper['journal_ref']}\\n\"\n",
    "            f\"DOI: {paper['doi']}\\n\"\n",
    "            f\"Primary Category: {paper['primary_category']}\\n\"\n",
    "            f\"Categories: {', '.join(paper['categories'])}\\n\"\n",
    "            f\"PDF URL: {paper['pdf_url']}\\n\"\n",
    "            f\"Arxiv URL: {paper['arxiv_url']}\\n\"\n",
    "        )\n",
    "        documents.append(Document(text=content))\n",
    "    return documents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='8c2a7701-d76a-4c40-9a16-e7423d6ff993', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark\\nAuthors: Sondos Mahmoud Bsharat, Mukul Ranjan, Aidar Myrzakhan, Jiacheng Liu, Bowei Guo, Shengkun Tang, Zhuang Liu, Yuanzhi Li, Zhiqiang Shen\\nSummary: Rapid advancements in large language models (LLMs) have increased interest in\\ndeploying them on mobile devices for on-device AI applications. Mobile users\\ninteract differently with LLMs compared to desktop users, creating unique\\nexpectations and data biases. Current benchmark datasets primarily target at\\nserver and desktop environments, and there is a notable lack of extensive\\ndatasets specifically designed for mobile contexts. Additionally, mobile\\ndevices face strict limitations in storage and computing resources,\\nconstraining model size and capabilities, thus requiring optimized efficiency\\nand prioritized knowledge. To address these challenges, we introduce\\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\\nsignificantly more difficult than our standard full set. Both benchmarks use\\nmultiple-choice, order-invariant questions focused on practical mobile\\ninteractions, such as recipe suggestions, travel planning, and essential daily\\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\\nlatency, energy consumption, memory usage, and response quality, offering\\ncomprehensive insights into model performance under mobile constraints.\\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\\nperform on-device processing, maintain user privacy, and adapt to personalized\\nusage patterns. Mobile-MMLU family offers a standardized framework for\\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\\nproductivity and decision-making within mobile computing environments. Our code\\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.\\nPublished: 2025-03-26 17:59:56+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI\\nPDF URL: http://arxiv.org/pdf/2503.20786v1\\nArxiv URL: http://arxiv.org/abs/2503.20786v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f76e72a8-a9aa-4eb7-a575-e397f0ddc8b9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: ASGO: Adaptive Structured Gradient Optimization\\nAuthors: Kang An, Yuxing Liu, Rui Pan, Shiqian Ma, Donald Goldfarb, Tong Zhang\\nSummary: Training deep neural networks (DNNs) is a structured optimization problem,\\nbecause the parameters are naturally represented by matrices and tensors rather\\nthan simple vectors. Under this structural representation, it has been widely\\nobserved that gradients are low-rank and Hessians are approximately block-wise\\ndiagonal. These structured properties are crucial for designing efficient\\noptimization algorithms but may not be utilized by current popular optimizers\\nlike Adam. In this paper, we present a novel optimization algorithm ASGO that\\ncapitalizes on these properties by employing a preconditioner that is\\nadaptively updated using structured gradients. By fine-grained theoretical\\nanalysis, ASGO is proven to achieve superior convergence rates compared to\\nexisting structured gradient methods. Based on the convergence theory, we\\nfurther demonstrate that ASGO can benefit from the low-rank and block-wise\\ndiagonal properties. We also discuss practical modifications of ASGO and\\nempirically verify the effectiveness of the algorithm on language model tasks.\\nPublished: 2025-03-26 17:50:13+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, math.OC\\nPDF URL: http://arxiv.org/pdf/2503.20762v1\\nArxiv URL: http://arxiv.org/abs/2503.20762v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f596194d-e5fa-4440-8d63-53509ee38156', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search\\nAuthors: Yunhai Hu, Yilun Zhao, Chen Zhao, Arman Cohan\\nSummary: We introduce MCTS-RAG, a novel approach that enhances the reasoning\\ncapabilities of small language models on knowledge-intensive tasks by\\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\\nintegrates retrieval and reasoning through an iterative decision-making\\nprocess. Unlike standard RAG methods, which typically retrieve information\\nindependently from reasoning and thus integrate knowledge suboptimally, or\\nconventional MCTS reasoning, which depends solely on internal model knowledge\\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\\nretrieval. This integrated approach enhances decision-making, reduces\\nhallucinations, and ensures improved factual accuracy and response consistency.\\nThe experimental results on multiple reasoning and knowledge-intensive datasets\\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\\nGPT-4o by effectively scaling inference-time compute, setting a new standard\\nfor reasoning in small-scale models.\\nPublished: 2025-03-26 17:46:08+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2503.20757v1\\nArxiv URL: http://arxiv.org/abs/2503.20757v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='22249a7d-5b50-4fbf-906d-6ae236ebe5fc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning\\nAuthors: Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang\\nSummary: Visual reasoning abilities play a crucial role in understanding complex\\nmultimodal data, advancing both domain-specific applications and artificial\\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\\ntraining data to enhance visual reasoning capabilities. However, this training\\nparadigm may lead to overfitting and cognitive rigidity, restricting the\\nmodel's ability to transfer visual reasoning skills across domains and limiting\\nits real-world applicability. To address these limitations, we propose\\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\\nmultiple reasoning-response pairs, significantly enhancing generalization in\\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\\nwe reconstructed a comprehensive dataset spanning visual counting, structure\\nperception, and spatial transformation.cExperimental results demonstrate\\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\\nstate-of-the-art results across multiple tasks, outperforming most mainstream\\nopen-source and proprietary models; (2) Generalization Superiority:\\nconsistently maintaining robust performance across diverse tasks and domains,\\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.\\nPublished: 2025-03-26 17:38:06+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.AI\\nPDF URL: http://arxiv.org/pdf/2503.20752v1\\nArxiv URL: http://arxiv.org/abs/2503.20752v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='694cd8de-5671-44f7-9422-54fa85778354', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams\\nAuthors: Yanpeng Sun, Shan Zhang, Wei Tang, Aotian Chen, Piotr Koniusz, Kai Zou, Yuan Xue, Anton van den Hengel\\nSummary: Diagrams serve as a fundamental form of visual language, representing complex\\nconcepts and their inter-relationships through structured symbols, shapes, and\\nspatial arrangements. Unlike natural images, their inherently symbolic and\\nabstract nature poses significant challenges for Multimodal Large Language\\nModels (MLLMs). However, current benchmarks conflate perceptual and reasoning\\ntasks, making it difficult to assess whether MLLMs genuinely understand\\nmathematical diagrams beyond superficial pattern recognition. To address this\\ngap, we introduce MATHGLANCE, a benchmark specifically designed to isolate and\\nevaluate mathematical perception in MLLMs. MATHGLANCE comprises 1.2K images and\\n1.6K carefully curated questions spanning four perception tasks: shape\\nclassification, object counting, relationship identification, and object\\ngrounding, covering diverse domains including plane geometry, solid geometry,\\nand graphical representations. Our evaluation of MLLMs reveals that their\\nability to understand diagrams is notably limited, particularly in fine-grained\\ngrounding tasks. In response, we construct GeoPeP, a perception-oriented\\ndataset of 200K structured geometry image-text pairs explicitly annotated with\\ngeometric primitives and precise spatial relationships. Training MLLM on GeoPeP\\nleads to significant gains in perceptual accuracy, which in turn substantially\\nimproves mathematical reasoning. Our benchmark and dataset establish critical\\nstandards for evaluating and advancing multimodal mathematical understanding,\\nproviding valuable resources and insights to foster future MLLM research.\\nPublished: 2025-03-26 17:30:41+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2503.20745v1\\nArxiv URL: http://arxiv.org/abs/2503.20745v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0b0955d0-4012-4328-a8d8-40ca1b9b74e8', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Dynamic Motion Blending for Versatile Motion Editing\\nAuthors: Nan Jiang, Hongjie Li, Ziye Yuan, Zimo He, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang\\nSummary: Text-guided motion editing enables high-level semantic control and iterative\\nmodifications beyond traditional keyframe animation. Existing methods rely on\\nlimited pre-collected training triplets, which severely hinders their\\nversatility in diverse editing scenarios. We introduce MotionCutMix, an online\\ndata augmentation technique that dynamically generates training triplets by\\nblending body part motions based on input text. While MotionCutMix effectively\\nexpands the training distribution, the compositional nature introduces\\nincreased randomness and potential body part incoordination. To model such a\\nrich distribution, we present MotionReFit, an auto-regressive diffusion model\\nwith a motion coordinator. The auto-regressive architecture facilitates\\nlearning by decomposing long sequences, while the motion coordinator mitigates\\nthe artifacts of motion composition. Our method handles both spatial and\\ntemporal motion edits directly from high-level human instructions, without\\nrelying on additional specifications or Large Language Models. Through\\nextensive experiments, we show that MotionReFit achieves state-of-the-art\\nperformance in text-guided motion editing.\\nPublished: 2025-03-26 17:07:24+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2503.20724v1\\nArxiv URL: http://arxiv.org/abs/2503.20724v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f9707ac6-7848-4fa9-bc0c-81ffbf3e0e03', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models\\nAuthors: Nikita Neveditsin, Pawan Lingras, Vijay Mago\\nSummary: This study examines the performance of Large Language Models (LLMs) in\\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\\na metric to facilitate the evaluation of aspect extraction with generative\\nmodels. Our findings highlight both the potential and limitations of LLMs in\\nthe ABSA task.\\nPublished: 2025-03-26 16:52:40+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2503.20715v1\\nArxiv URL: http://arxiv.org/abs/2503.20715v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6491f07b-b06d-4b85-aa9a-91cf3be26f02', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion\\nAuthors: Saron Samuel, Dan DeGenaro, Jimena Guallar-Blasco, Kate Sanders, Oluwaseun Eisape, Arun Reddy, Alexander Martin, Andrew Yates, Eugene Yang, Cameron Carpenter, David Etter, Efsun Kayi, Matthew Wiesner, Kenton Murray, Reno Kriz\\nSummary: Videos inherently contain multiple modalities, including visual events, text\\noverlays, sounds, and speech, all of which are important for retrieval.\\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\\nare built on vision-language models (VLMs), and thus overly prioritize visual\\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\\nqueries and neglecting other modalities. We create a search system MMMORRF that\\nextracts text and features from both visual and audio modalities and integrates\\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\\nboth effective and efficient, demonstrating practicality in searching videos\\nbased on users' information needs instead of visual descriptive queries. We\\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\\nover leading multimodal encoders and 37% over single-modality retrieval,\\ndemonstrating the value of integrating diverse modalities.\\nPublished: 2025-03-26 16:28:04+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.IR\\nPDF URL: http://arxiv.org/pdf/2503.20698v1\\nArxiv URL: http://arxiv.org/abs/2503.20698v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a2ca1f07-68e9-44bd-9adf-3ab057ec7d4f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy\\nAuthors: Yinan Sun, Xiongkuo Min, Zicheng Zhang, Yixuan Gao, Yuqin Cao, Guangtao Zhai\\nSummary: The rapid development of multimodal large language models has resulted in\\nremarkable advancements in visual perception and understanding, consolidating\\nseveral tasks into a single visual question-answering framework. However, these\\nmodels are prone to hallucinations, which limit their reliability as artificial\\nintelligence systems. While this issue is extensively researched in natural\\nlanguage processing and image captioning, there remains a lack of investigation\\nof hallucinations in Low-level Visual Perception and Understanding (HLPU),\\nespecially in the context of image quality assessment tasks. We consider that\\nthese hallucinations arise from an absence of clear self-awareness within the\\nmodels. To address this issue, we first introduce the HLPU instruction\\ndatabase, the first instruction database specifically focused on hallucinations\\nin low-level vision tasks. This database contains approximately 200K\\nquestion-answer pairs and comprises four subsets, each covering different types\\nof instructions. Subsequently, we propose the Self-Awareness Failure\\nElimination (SAFEQA) model, which utilizes image features, salient region\\nfeatures and quality features to improve the perception and comprehension\\nabilities of the model in low-level vision tasks. Furthermore, we propose the\\nEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase\\nthe model's awareness of knowledge boundaries, thereby mitigating the incidence\\nof hallucination. Finally, we conduct comprehensive experiments on low-level\\nvision tasks, with the results demonstrating that our proposed method\\nsignificantly enhances self-awareness of the model in these tasks and reduces\\nhallucinations. Notably, our proposed method improves both accuracy and\\nself-awareness of the proposed model and outperforms close-source models in\\nterms of various evaluation metrics.\\nPublished: 2025-03-26 16:05:01+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2503.20673v1\\nArxiv URL: http://arxiv.org/abs/2503.20673v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7dbdb2f5-21d0-4496-9da9-95d942f445bc', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews\\nAuthors: Huimin Xu, Seungjun Yi, Terence Lim, Jiawei Xu, Andrew Well, Carlos Mery, Aidong Zhang, Yuji Zhang, Heng Ji, Keshav Pingali, Yan Leng, Ying Ding\\nSummary: Thematic analysis (TA) is a widely used qualitative approach for uncovering\\nlatent meanings in unstructured text data. TA provides valuable insights in\\nhealthcare but is resource-intensive. Large Language Models (LLMs) have been\\nintroduced to perform TA, yet their applications in healthcare remain\\nunexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis\\nframework using Multi-Agent LLMs for clinical interviews. We leverage the\\nscalability and coherence of multi-agent systems through structured\\nconversations between agents and coordinate the expertise of cardiac experts in\\nTA. Using interview transcripts from parents of children with Anomalous Aortic\\nOrigin of a Coronary Artery (AAOCA), a rare congenital heart disease, we\\ndemonstrate that TAMA outperforms existing LLM-assisted TA approaches,\\nachieving higher thematic hit rate, coverage, and distinctiveness. TAMA\\ndemonstrates strong potential for automated TA in clinical settings by\\nleveraging multi-agent LLM systems with human-in-the-loop integration by\\nenhancing quality while significantly reducing manual workload.\\nPublished: 2025-03-26 15:58:16+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.HC\\nCategories: cs.HC, cs.CL\\nPDF URL: http://arxiv.org/pdf/2503.20666v1\\nArxiv URL: http://arxiv.org/abs/2503.20666v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5512f34f-1b38-4313-96dc-3c3980818cdb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language Model for Lung Nodule Malignancy Prediction\\nAuthors: Sadaf Khademi, Mehran Shabanpour, Reza Taleei, Anastasia Oikonomou, Arash Mohammadi\\nSummary: Lung cancer remains one of the leading causes of cancer-related mortality\\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\\ncases with similar visual characteristics and closely annotation scores. In\\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\\nfeatures extracted from Computed Tomography (CT) images, while recent research\\nhas primarily focused on deep learning solutions. More recently,\\nVision-Language Models (VLMs), particularly Contrastive Language-Image\\nPre-Training (CLIP)-based models, have gained attention for their ability to\\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\\nhave shown promising results, we identified the following potential\\nlimitations: (a) dependence on radiologists' annotated attributes, which are\\ninherently subjective and error-prone, (b) use of textual information only\\nduring training, limiting direct applicability at inference, and (c)\\nConvolutional-based vision encoder with randomly initialized weights, which\\ndisregards prior knowledge. To address these limitations, we introduce\\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\\nmulti-modal autoregressive objective. Given that lung tumors are typically\\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\\noffers significant advantages over its CLIP-based counterparts by capturing\\npixel-level differences. Additionally, we introduce conditional context\\noptimization, which dynamically generates context-specific prompts based on\\ninput Radiomics, improving cross-modal alignment.\\nPublished: 2025-03-26 15:56:48+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.LG, eess.IV\\nPDF URL: http://arxiv.org/pdf/2503.20662v1\\nArxiv URL: http://arxiv.org/abs/2503.20662v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b2fb278a-7a00-427e-8eee-940bdfae5c84', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports\\nAuthors: Xiangwen Zhang, Qian Zhang, Longfei Han, Qiang Qu, Xiaoming Chen\\nSummary: Collecting real-world vehicle accident videos for autonomous driving research\\nis challenging due to their rarity and complexity. While existing driving video\\ngeneration methods may produce visually realistic videos, they often fail to\\ndeliver physically realistic simulations because they lack the capability to\\ngenerate accurate post-collision trajectories. In this paper, we introduce\\nAccidentSim, a novel framework that generates physically realistic vehicle\\ncollision videos by extracting and utilizing the physical clues and contextual\\ninformation available in real-world vehicle accident reports. Specifically,\\nAccidentSim leverages a reliable physical simulator to replicate post-collision\\nvehicle trajectories from the physical and contextual information in the\\naccident reports and to build a vehicle collision trajectory dataset. This\\ndataset is then used to fine-tune a language model, enabling it to respond to\\nuser prompts and predict physically consistent post-collision trajectories\\nacross various driving scenarios based on user descriptions. Finally, we employ\\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\\nwith the foreground vehicles that exhibit physically realistic trajectories to\\ngenerate vehicle collision videos. Experimental results demonstrate that the\\nvideos produced by AccidentSim excel in both visual and physical authenticity.\\nPublished: 2025-03-26 15:50:42+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV, cs.AI\\nPDF URL: http://arxiv.org/pdf/2503.20654v1\\nArxiv URL: http://arxiv.org/abs/2503.20654v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='eafd991b-9d6b-4bcb-9fbc-de52a3ff67b0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging\\nAuthors: Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan\\nSummary: The transition from System 1 to System 2 reasoning in large language models\\n(LLMs) has marked significant advancements in handling complex tasks through\\ndeliberate, iterative thinking. However, this progress often comes at the cost\\nof efficiency, as models tend to overthink, generating redundant reasoning\\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\\nreasoning has emerged as a promising solution to this challenge, aiming to\\nbalance reasoning depth with practical efficiency. While existing approaches,\\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\\nengineering, have shown potential, they are either computationally expensive or\\nunstable. Model merging, on the other hand, offers a cost-effective and robust\\nalternative by integrating the quick-thinking capabilities of System 1 models\\nwith the methodical reasoning of System 2 models. In this work, we present a\\ncomprehensive empirical study on model merging for L2S reasoning, exploring\\ndiverse methodologies, including task-vector-based, SVD-based, and\\nactivation-informed merging. Our experiments reveal that model merging can\\nreduce average response length by up to 55% while preserving or even improving\\nbaseline performance. We also identify a strong correlation between model scale\\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\\nFurthermore, we investigate the merged model's ability to self-critique and\\nself-correct, as well as its adaptive response length based on task complexity.\\nOur findings highlight model merging as a highly efficient and effective\\nparadigm for L2S reasoning, offering a practical solution to the overthinking\\nproblem while maintaining the robustness of System 2 reasoning. This work can\\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.\\nPublished: 2025-03-26 15:34:37+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2503.20641v1\\nArxiv URL: http://arxiv.org/abs/2503.20641v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d926aab8-e066-4ed2-9c4d-77898ee25e84', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions\\nAuthors: Alessandro Maisto\\nSummary: Role-playing games (RPG) are games in which players interact with one another\\nto create narratives. The role of players in the RPG is largely based on the\\ninteraction between players and their characters. This emerging form of shared\\nnarrative, primarily oral, is receiving increasing attention. In particular,\\nmany authors investigated the use of an LLM as an actor in the game. In this\\npaper, we aim to discover to what extent the language of Large Language Models\\n(LLMs) exhibit oral or written features when asked to generate an RPG session\\nwithout human interference. We will conduct a linguistic analysis of the\\nlexical and syntactic features of the generated texts and compare the results\\nwith analyses of conversations, transcripts of human RPG sessions, and books.\\nWe found that LLMs exhibit a pattern that is distinct from all other text\\ncategories, including oral conversations, human RPG sessions and books. Our\\nanalysis has shown how training influences the way LLMs express themselves and\\nprovides important indications of the narrative capabilities of these tools.\\nPublished: 2025-03-26 15:10:47+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL, cs.AI\\nPDF URL: http://arxiv.org/pdf/2503.20623v1\\nArxiv URL: http://arxiv.org/abs/2503.20623v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='943d27b1-b15d-4d82-8302-ca5133922d05', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: IAP: Improving Continual Learning of Vision-Language Models via Instance-Aware Prompting\\nAuthors: Hao Fu, Hanbin Zhao, Jiahua Dong, Chao Zhang, Hui Qian\\nSummary: Recent pre-trained vision-language models (PT-VLMs) often face a Multi-Domain\\nClass-Incremental Learning (MCIL) scenario in practice, where several classes\\nand domains of multi-modal tasks are incrementally arrived. Without access to\\npreviously learned tasks and unseen tasks, memory-constrained MCIL suffers from\\nforward and backward forgetting. To alleviate the above challenges,\\nparameter-efficient fine-tuning techniques (PEFT), such as prompt tuning, are\\nemployed to adapt the PT-VLM to the diverse incrementally learned tasks. To\\nachieve effective new task adaptation, existing methods only consider the\\neffect of PEFT strategy selection, but neglect the influence of PEFT parameter\\nsetting (e.g., prompting). In this paper, we tackle the challenge of optimizing\\nprompt designs for diverse tasks in MCIL and propose an Instance-Aware\\nPrompting (IAP) framework. Specifically, our Instance-Aware Gated Prompting\\n(IA-GP) module enhances adaptation to new tasks while mitigating forgetting by\\ndynamically assigning prompts across transformer layers at the instance level.\\nOur Instance-Aware Class-Distribution-Driven Prompting (IA-CDDP) improves the\\ntask adaptation process by determining an accurate task-label-related\\nconfidence score for each instance. Experimental evaluations across 11\\ndatasets, using three performance metrics, demonstrate the effectiveness of our\\nproposed method. Code can be found at https://github.com/FerdinandZJU/IAP.\\nPublished: 2025-03-26 14:59:23+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CV\\nCategories: cs.CV\\nPDF URL: http://arxiv.org/pdf/2503.20612v1\\nArxiv URL: http://arxiv.org/abs/2503.20612v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3fab3779-b36f-4907-9c78-56e1fc118046', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical Study and Beyond\\nAuthors: Wenchao Gu, Juntao Chen, Yanlin Wang, Tianyue Jiang, Xingzhe Li, Mingwei Liu, Xilin Liu, Yuchi Ma, Zibin Zheng\\nSummary: Repository-level code generation remains challenging due to complex code\\ndependencies and the limitations of large language models (LLMs) in processing\\nlong contexts. While retrieval-augmented generation (RAG) frameworks are widely\\nadopted, the effectiveness of different retrieved information\\nsources-contextual code, APIs, and similar snippets-has not been rigorously\\nanalyzed. Through an empirical study on two benchmarks, we demonstrate that\\nin-context code and potential API information significantly enhance LLM\\nperformance, whereas retrieved similar code often introduces noise, degrading\\nresults by up to 15%. Based on the preliminary results, we propose\\nAllianceCoder, a novel context-integrated method that employs chain-of-thought\\nprompting to decompose user queries into implementation steps and retrieves\\nAPIs via semantic description matching. Through extensive experiments on\\nCoderEval and RepoExec, AllianceCoder achieves state-of-the-art performance,\\nimproving Pass@1 by up to 20% over existing approaches.\\nPublished: 2025-03-26 14:41:38+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.SE\\nCategories: cs.SE\\nPDF URL: http://arxiv.org/pdf/2503.20589v1\\nArxiv URL: http://arxiv.org/abs/2503.20589v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='caa34596-91d4-4182-8283-33650576834e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: LLPut: Investigating Large Language Models for Bug Report-Based Input Generation\\nAuthors: Alif Al Hasan, Subarna Saha, Mia Mohammad Imran, Tarannum Shaila Zaman\\nSummary: Failure-inducing inputs play a crucial role in diagnosing and analyzing\\nsoftware bugs. Bug reports typically contain these inputs, which developers\\nextract to facilitate debugging. Since bug reports are written in natural\\nlanguage, prior research has leveraged various Natural Language Processing\\n(NLP) techniques for automated input extraction. With the advent of Large\\nLanguage Models (LLMs), an important research question arises: how effectively\\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\\npaper, we propose LLPut, a technique to empirically evaluate the performance of\\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\\nextracting relevant inputs from bug reports. We conduct an experimental\\nevaluation on a dataset of 206 bug reports to assess the accuracy and\\neffectiveness of these models. Our findings provide insights into the\\ncapabilities and limitations of generative LLMs in automated bug diagnosis.\\nPublished: 2025-03-26 14:25:01+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.SE\\nCategories: cs.SE\\nPDF URL: http://arxiv.org/pdf/2503.20578v1\\nArxiv URL: http://arxiv.org/abs/2503.20578v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b6643636-e39e-440e-a409-cf7ae45bf4a7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models\\nAuthors: Siyuan Guo, Huiwu Liu, Xiaolong Chen, Yuming Xie, Liang Zhang, Tao Han, Hechang Chen, Yi Chang, Jun Wang\\nSummary: In this work, we explore the potential of large language models (LLMs) for\\ngenerating functional test scripts, which necessitates understanding the\\ndynamically evolving code structure of the target software. To achieve this, we\\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\\nof test intent descriptions and corresponding test scripts to facilitate LLMs\\nfor test script generation. To improve user experience further, we introduce\\nRe4, an optimization method for the CBR system, comprising reranking-based\\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\\nidentify positive examples with high semantic and script similarity, providing\\nreliable pseudo-labels for finetuning the retriever model without costly\\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\\nlearning finetuning stage, to align LLMs with our production scenarios,\\nensuring the faithful reuse of retrieved cases. Extensive experimental results\\non two product development units from Huawei Datacom demonstrate the\\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\\nRe4 method can help alleviate the repetitive generation issues with LLMs.\\nPublished: 2025-03-26 14:23:59+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.SE\\nCategories: cs.SE, cs.CL, cs.LG\\nPDF URL: http://arxiv.org/pdf/2503.20576v1\\nArxiv URL: http://arxiv.org/abs/2503.20576v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b4c83df6-611e-4748-bd91-25387457bcd6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Low-resource Information Extraction with the European Clinical Case Corpus\\nAuthors: Soumitra Ghosh, Begona Altuna, Saeed Farzi, Pietro Ferrazzi, Alberto Lavelli, Giulia Mezzanotte, Manuela Speranza, Bernardo Magnini\\nSummary: We present E3C-3.0, a multilingual dataset in the medical domain, comprising\\nclinical cases annotated with diseases and test-result relations. The dataset\\nincludes both native texts in five languages (English, French, Italian, Spanish\\nand Basque) and texts translated and projected from the English source into\\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\\nsemi-automatic approach has been implemented, including automatic annotation\\nprojection based on Large Language Models (LLMs) and human revision. We present\\nseveral experiments showing that current state-of-the-art LLMs can benefit from\\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\\ndifferent languages is very effective, mitigating the scarcity of data.\\nFinally, we compare performance both on native data and on projected data. We\\nrelease the data at\\nhttps://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89 .\\nPublished: 2025-03-26 14:07:40+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.CL\\nCategories: cs.CL\\nPDF URL: http://arxiv.org/pdf/2503.20568v1\\nArxiv URL: http://arxiv.org/abs/2503.20568v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0861e363-5e9b-456a-a45a-2d50a86b118f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: A Theoretical Framework for Prompt Engineering: Approximating Smooth Functions with Transformer Prompts\\nAuthors: Ryumei Nakada, Wenlong Ji, Tianxi Cai, James Zou, Linjun Zhang\\nSummary: Prompt engineering has emerged as a powerful technique for guiding large\\nlanguage models (LLMs) toward desired responses, significantly enhancing their\\nperformance across diverse tasks. Beyond their role as static predictors, LLMs\\nincreasingly function as intelligent agents, capable of reasoning,\\ndecision-making, and adapting dynamically to complex environments. However, the\\ntheoretical underpinnings of prompt engineering remain largely unexplored. In\\nthis paper, we introduce a formal framework demonstrating that transformer\\nmodels, when provided with carefully designed prompts, can act as a\\nconfigurable computational system by emulating a ``virtual'' neural network\\nduring inference. Specifically, input prompts effectively translate into the\\ncorresponding network configuration, enabling LLMs to adjust their internal\\ncomputations dynamically. Building on this construction, we establish an\\napproximation theory for $\\\\beta$-times differentiable functions, proving that\\ntransformers can approximate such functions with arbitrary precision when\\nguided by appropriately structured prompts. Moreover, our framework provides\\ntheoretical justification for several empirically successful prompt engineering\\ntechniques, including the use of longer, structured prompts, filtering\\nirrelevant information, enhancing prompt token diversity, and leveraging\\nmulti-agent interactions. By framing LLMs as adaptable agents rather than\\nstatic models, our findings underscore their potential for autonomous reasoning\\nand problem-solving, paving the way for more robust and theoretically grounded\\nadvancements in prompt engineering and AI agent design.\\nPublished: 2025-03-26 13:58:02+00:00\\nJournal Reference: None\\nDOI: None\\nPrimary Category: cs.LG\\nCategories: cs.LG, stat.ML\\nPDF URL: http://arxiv.org/pdf/2503.20561v1\\nArxiv URL: http://arxiv.org/abs/2503.20561v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings,VectorStoreIndex\n",
    "from constants import embed_model \n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 50\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(\"index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
